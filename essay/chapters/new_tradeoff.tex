
We have argued that most of the fairness trade-offs identified at the intersection of ML and medicine are not new, but rather that the preexisting ones are preserved, increased or possibly decreased. However, the deployment of ML systems in the medical context does introduce new trade-offs into medicine apart from the fairness domain. This might involve trade-offs which uniquely emerge from the technology of medical ML \cite{Dijkstra2020}, so in the following we will zoom out of the fairness domain to see which problems occur when ML and medicine are combined. 


\subsection{Multi-faceted trade-offs everywhere}

ML applications in medicine are often discussed as a human vs. machine situation - where humans are outperformed by medical ML systems, they should and in the near future will be substituted. However, this situation creates a binary decision that is hard to make, especially with ML systems which can involve a good amount of uncertainty. It also creates an environment where humans are competing with machines for the prerogative of interpretation, which contains an understanding of ML systems as some kind of autonomous systems. This autonomy is rather imaginary since today, medical ML systems still need their decision making process to be started and evaluated by humans. Thus, we are left with a distinction between ML systems as a tool or a machine as it is described in \cite{Williamson2021} and as it is argued there, this distinction is often made based on familiarity - new developments are machines and will only be called tools once they grow older and people get used to them. 

Recently, different studies found that combining ML and human evaluation can achieve better results than either of the two on their own \cite{rajpurkar2022ai, kiani2020impact, topol2019high, steiner2018impact}. This suggests that human practitioners should use medical ML systems to support their decisions and increase their efficiency rather than be replaced by them, which is in line with understanding medical ML systems as tools. So instead of a binary decision we are left with a new situation that fits our understanding of trade-offs. How are ML and human evaluation best combined to achieve the best accuracy, how much human involvement do we want or need? This might heavily depend on the task at hand. For example, for skin cancer classification where the input is only a cropped image of the potential carcinoma or melanoma, the algorithms decision alone might be enough. However, for identifying diseases in a breast X-ray, a much broader task than skin cancer classification, algorithmic and human judgment might need to be combined for the optimal solution.  One of the studies mention above found that especially for harder cases, the assisted accuracy was very high compared to the unassisted accuracy when the ML model's prediction was correct, but that it was also painfully low in cases where the ML model's prediction was incorrect \cite{kiani2020impact}. Moreover, there is also evidence that medical ML systems might especially improve the performance of less experienced practitioners like those who are still in training while those who are already experienced would not profit as much \cite{rajpurkar2022ai}. This brings another level into the trade-off because with a necessitation to use the ML system, experienced practitioners who already perform on a similar level as the system might even be hindered by another step in their workflow or a confusing, wrong judgment by the ML system and thus, the overall performance could decrease. In fact, for CDSS without ML components it was already observed that more experienced doctors ignore their assistance more often without performing less good \cite{sutton2020overview}.

This trade-off is further complicated by the question of responsibility. Naturally, the more ML system and physician interact the harder it gets to identify where potential mistakes come from and thus understand whether a mistake by the practitioner or an error in the medical ML system is to blame \cite{horgan2019artificial}. This can be problematic since many applications of medical ML systems involve high-stakes scenarios where errors should be avoided at all cost or if they happen should be eradicated as fast as possible.

A clear responsibility framework is also important for fostering trust in the medical profession which leads us to the next trade-off. For a good relationship between patient and health care practitioner, trust is of the utmost importance \cite{clark2002trust}. For the application of ML in medicine and health we can identify a multi-faceted trade-off between trust in the system and accuracy that already starts with what we just mentioned, but continues far beyond that. It involves a trade-off that is standard to ML but grows to great importance especially in applications like medical ML systems, namely the trade-off between explainability and accuracy \cite{topol2019high, kelly2019key}. Most medical ML systems today perform worse as soon as some kind of interpretability framework is built in, leading to the question of how important explainability is for the application \cite{luo2019balancing}. While it is not yet clear how an explainability framework does influence the work of practitioners, studies have shown that it would increase trust in the system, from the practitioner's as well as from the patient's side \cite{diprose2020physician, topol2019high}. It seems that explainability might be a decent solution to gain the trust needed, but it could in turn also worsen the accuracy, thus actually making the distrust in the technology more reasonable.

This can be related to another, rather philosophical level of this trade-off: If a person is skeptical about using ML on their diagnostic case, how can we trade off a potentially better diagnosis against respecting the persons wish, which could lead to a less accurate or even wrong diagnosis? Currently, there is no right answer to this question, it will depend on what society and the medical profession think is more important. Nonetheless, it is questionable how much the current health care workforce would at all be able to foster trust in ML systems in such a case. Not only because of the missing explanations by the system itself, but also because most practitioners currently lack the education in ML technologies \cite{he2019practical}. How much of an education is necessary is of course debatable, but it might be one possibility to increase an understanding of the output of ML systems without trading off too much accuracy against explainability.

There are other suggestions of how to foster trust in medical ML systems, for example by increasing transparency about the decision. If it is impossible to reach a conclusion, perhaps because there is not enough data, ML systems should be transparent about that and indicate that they cannot make a decision rather than making a badly informed one \cite{horgan2019artificial}. 

In recent years and especially with the COVID-19 pandemic the current health care systems in place have taken a huge toll on the mental health of care workers around the world \cite{vizheh2020mental}. One main problem is the increasing lack of qualified personal in combination with a surge in hospitalizations during the pandemic. And while the pandemic acted as an accelerator, even without it hospitalization rates would have been likely to increase over the next years due to the current shift in age demographics in the western world. Medical ML systems are said to be able to increase efficiency in hospitals and prevent unnecessary hospital visits, thus reducing pressure on care workers and doctors, something that would certainly be seen as a positive development \cite{horgan2019artificial}. However, it will be important to take a holistic approach towards health care in the future. Too often, medical ML systems are seen as the holy grail to solve problems when - as we have argued before - they are just tools that will not tackle structural problems without being used to do so. For example, in most current health care systems reduced workload for care workers would potentially lead to a reduction of jobs because this way, money can be saved. However, this would then not lead to an actual improvement for patients but only to a potential financial reward for the medical institution. Here, we can identify a trade-off between monetary outcomes and spendings on the one and the patients experience and care on the other hand. While this is an issue that is already existing, medical ML systems bring another perspective to it since they have the potential to increase as well as heavily decrease the patients experience in hospitals. 
This might also include trading off potentially more empathetic, person-to-person interactions in treatment against a more standardized way of tackling tasks through medical ML systems \cite{Morley2020}. As before, the question can be raised what we understand as good health care - is it only about the right diagnosis or is psychological well-being during treatment, for example, important as well? 

On the other hand, if done right medical ML systems can certainly be used to take some mental load off the shoulders of human practitioners. They could, for example, be used to make decisions in situations where triage is needed since those decisions should by definition be made only on the basis of numbers, namely who is more likely to survive \cite{topol2019high, he2019practical}. Of course, this again raises the question whether it is ethical to predict the likelihood of death using computers.

But let us continue with the topic of funding. The development and deployment of ML systems in medicine and health care will and does already cost a lot of money \cite{he2019practical}. At the same time, the health care system in general in countries like the US is heavily underfunded. So much so that live expectancy began to decrease again in the US \cite{topol2019high}. As we have seen before, this does not affect all parts of the society in the same way (see Section 3.2). Thus, there can be a trade-off identified between the financing of medical ML systems and the health care system in general. If the huge investments in medical ML systems will only benefit a small wealthier part of society, those investments are questionable if the health care systems continue to be underfunded. This is even more the case since there are not yet many medical ML systems ready for clinical application which makes this money an investment into the future while there persist acute issues that would need to be tackled here and now.

This is also related to the issue of privatization. Health care systems around the world are already more or less privatized, with the degree of privatization depending on the country. However, in the case of medical ML systems a lot of research and development is driven by big companies like Alphabet or IBM \cite{Morley2020}. This makes sense since those companies are driving ML research in general, but it poses the question whether we want to give such an important issue completely out of public and into private hands, potentially even risking monopolization. While the privatization of health care was already posing problems before the rise of medical ML systems and they are in fact seen as a solution for the existing problems, privacy and trust in the technology seem to suffer from the involvement of big tech as well \cite{Morley2020, topol2019high}. Thus, this can be seen as a trade-off between the speed of development - arguably, big tech companies will be fast in bringing medical ML systems to the market - and keeping privacy and - again - trust issues as small as possible. The importance of this perspective increases even further when we think about the way data is handled in traditional medicine as opposed to ML \cite{he2019practical}. Unfortunately, to make medical ML systems work properly there is a need for huge amounts of data that will be shared with the respective companies and researchers. This creates a trade-off between the classical handling of medical data and a necessary data collection for ML. It also plays into the discussion about big tech companies since they are rarely seen as faithful recipients of our data and extending their range to our medical data could certainly increase privacy concerns and thus decrease trust.

Apart from who is developing medical ML systems their development raises another important question: How do we verify that they are working and how should their deployment be regulated? This again leads to a multi-faceted trade-off between tried and trusted regulation processes for medical applications, trust in those systems, and the speed of development in medical ML systems. In general, software systems are constantly updated after deployment - however, with medical ML systems this poses the problem that new updates with new training data might change the way of functioning significantly and that every new update should be evaluated in terms of fairness and overall ethical performance \cite{he2019practical}. The fact that ML systems can evolve after receiving their clinical approval has even led to some scholars questioning the current gold standard of randomized controlled trials as the right tool for evaluation \cite{genin2021randomized}.
How should the agile updating be weighed against traditional and more accurate, but slower ways of approving tools for clinical application? In the US, the FDA already reacted by creating easier paths for updating this kind of system, but the success of this pathway is still indeterminate. Currently, regulation processes are often such that the model is locked in place before deployment. This makes it easier to regulate them, but misses out on their potential to learn and increase functionality on the fly. 


\subsection{Other technologies as inspiration}


The list of trade-offs could of course be extended further. Some scholars argue that while a mistake by a human practitioner only affects a small amount of people (possibly only one person), a mistake by an algorithm that is deployed in many hospitals will be replicated more often and thus affect more people \cite{Morley2020}. This could be seen as a trade-off between the scale of deployment of a technology and the severity of mistakes. However, this argument can also be seen as flawed since although one practitioner might not repeat a certain mistake, other practitioners not involved in this situation might well do. Thus, the only argumentation here could be that the errors are not as systematically spread as with medical ML systems, although even that might be an overstatement. One could also start talking about the fact that medical ML systems currently only work well for very specific tasks, i.e. detecting one or a couple of diseases in an X-ray. While the accuracy rate here is often high, the broadness of the analysis is very limited compared to a doctor \cite{topol2019high, rajpurkar2022ai}. This could be identified as a trade-off between high accuracy with a narrow focus on the one hand and lower accuracy with a broader focus on the other. 


However, the trade-offs detailed above are already sufficient to identify important patterns. Many of the trade-offs discussed can essentially be broken down to one question: How much do we benefit from the use of ML in medicine? What might be bad for us, for example could the digitalization and sharing of our health data lead to misusage by health care providers or tech companies? If I know that I will most probably benefit from sharing my data on the other hand, I will be more likely to do so \cite{topol2019high, he2019practical}. When talking about fairness measures in ML we mentioned that some measures are proven to be mutually exclusive. Even though it would be hard to prove so mathematically, it is very likely that some benefits of medical ML systems will be mutually exclusive as well and that going for one benefit might increase some problematic outcome on the other side. As for fairness, it is important to understand that there will never be a perfect solution for ethical trade-offs that fits everyone.

The question remains to answer whether the trade-offs mentioned in this chapter are actually new. We already mentioned earlier that ML should rather be understood as a tool than as a machine. So can we maybe learn from the history of other tools with regards to how trade-offs could be handled? It is certainly the case that many trade-offs that are presented here can be related to new technologies in general, for example trust issues. Especially medicine is a field were new tools are introduced over and over again and there has been a digitalization wave before \cite{sutton2020overview}. Thus, it will be possible to take inspiration from the past and the ways comparable trade-offs were tackled at the advent of other technologies to understand which problems we can approach the same way and which problems we might have to approach differently. 




%Are those trade-offs actually new or can we learn from history?
%Certainly, many of the trade-offs can be related to new technologies in general \cite{Williamson2021}
%Digitalization in medicine is not new - we can learn from how comparable trade-offs were tackled in the past \cite{sutton2020overview}


%But this also leads to the question whether these trade-offs are actually new or - as discussed for fairness trade-offs above - if they are just new editions of trade-offs humanity has seen before in either ML, medicine and health care or technology in general.

%This is certainly not only a problem in medicine, but medicine has always been a discipline where trust is particularly important. Machine Learning will bring a new twist to this issue and medicine is in that sense a unique challenge for Machine Learning.



%This would create a very general trade-off between benefits and caveats of ML tools in medicine and health care .
%Can digital health care be for everybody? what about people who do not have digital devices or don't want to use them?


%%Arthur:
%In particular, the use of ML as assisting systems rather than replacements of clinicians altogether complicates the discussion about biases further.
%The end effect of the integration of ML tools in medical practice is a complex function of the interaction of their results and their usage by clinicians on patients \cite[p.~4]{Rajkomar2018}.