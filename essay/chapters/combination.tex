\subsection{Old and new problems}
As discussed in the previous sections, the practical problem of applying Machine Learning to health care tasks carries with it a certain number of unavoidable decisions about the relative importance of contrasting principles.
In short, trade-offs have to be made.
In this essay, we concentrate on trade-offs related with fairness considerations.
An interesting aspect of such trade-offs is their origin: what are their causes?
We argue that most, if not all, fairness-related trade-offs originate from the decision (medical) problem itself, and not from the technology used to solve it.
\idea{Look for resources in Bob's technology lecture.}
That is, Machine Learning does not introduce additional fairness problems, but simply specifies existing trade-offs and makes them unavoidable.

As a concrete example, take the problem of unbalanced training data causing the Machine Learning algorithm to reach better performance on over-represented groups.
This problem is hidden, but still present, in unassisted medicine.
Medical practitioners arguably learn the best treatments partly from experience.
If the majority of their patients come from a particular group, it is very likely that they will ``overfit'' their knowledge to that group, or at least be able to predict their response to particular treatments better.
Similarly, textbook knowledge is derived from observations from medical practitioners and/or statistical studies.
Biased data informing those studies will bias the observed results.
This effect is exemplified by hearth health research, where research on biased data (higher proportion of men) lead to uneven improvements in heart attacks treatment with respect to gender \cite[e221]{Mccradden2020}.
The data that informs that knowledge is probably very similar to the data used to train ML algorithms.
The resulting inferences will hence be similarly biased as a consequence of data imbalances.
In this sense, ML systems even have a better potential to solve the problem, using for example importance weighting or under-/over-sampling \cite[pp.~6-8]{Chen2021}.
By no means do we intend to suggest that the solution is easy, since blindly applied technical fixes may introduce undetected harms (contrasting with the bioethical principle of non-maleficence).
However, ML has the potential to fix biases in a way that traditional medical practice can not \cite[p.~e222]{Mccradden2020} (although traditional statistics can, see e.~g.~importance sampling).

Similarly, the problem of data privacy pre-exists the entry of Machine Learning in the medical field \cite[p.~346]{Dijkstra2020}.
Privilege bias (models being developed for diseases that disproportionately affect a certain group) \cite[p.~5]{Rajkomar2018} is a problem that exists in classical statistical studies as well \cite{Jackson2019}.
In short, the trade-offs we analyzed (group fairness and individual fairness, privacy and predictability, fairness and predictability) exist independently of the system used to make decisions.
That is, they are not inherent to the technology used to solve them, ML, but to the goals and requirements of the system.
ML can be used to proactively advance health equity (beneficence), and not only avoiding harms (non-maleficence) \cite[p.~2]{Rajkomar2018}.

We do not, however, argue that ML does not introduce any new ethical problems, but that they are not fairness-related.
For example, ML systems applied at a large scale unify decisions, exponentially increasing the impact of failures.
Less federated decisions make for less error-robust systems, and unified treatment and strategy gives fewer indications about what works well, potentially reducing the possibilities for learning from single experiences.
Additionally, automated decision systems distribute the responsibility for the decisions the system takes, making it very difficult to attribute responsibility for potential misjudgments \cite[p.~6]{Morley2020}.
If the decisions are not taken automatically, the practitioner might still rely too much on them and avoid a right call that would contradict the suggestion of the ML system \cite[p.~4]{Morley2020} (automation bias, \cite[p.~4]{Rajkomar2018}).
Additionally, they might pay less attention to the decisions that are assisted by technology.
The lack of explainability of the decisions taken by automated systems might additionally contrast with the biomedical principle of the respect of autonomy, since it reduces the patient's possibility to exert informed consent \cite[p.~346]{Dijkstra2020}.
Finally, the trust of the public in the decisions taken by automated decision systems may be low, making their large-scale use politically difficult \cite[p.~4]{Morley2020}.


\subsection{Clarification of problems}
\idea{
    Main point: the mathematical rigor of ML forces us to think about those problems; this is a positive feature and not a disadvantage.
}
How can ML actively help advance health equity and fairness?
Firstly, it imposes the need for precise definitions of what is meant by terms like ``discrimination'', ``equity'', and so on.
Secondly, it forces the developers of the system to choose precise weights for the principles that they want to respect, and explicitly accept the existence of trade-offs that are inherent to the problem.
Thirdly, it makes the goals and evaluation metrics (and their implied definition of what a ``good'' solution looks like) clear.
Knowing that those goals influence the results, with for example pure efficiency potentially leading to the propagation of health inequities \cite[p.~2]{Rajkomar2018}, the importance of each objective has to be decided upon (and hence, the chosen position on the Pareto frontier).



\subsection{Advantage of inaction}
\idea{
    \begin{itemize}
        \item Positive versus negative harms: in doubt, do nothing.
        \item This reasoning is much harder to apply to critical problems as those emerging in medicine.
    \end{itemize}
}
